<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>大模型的loss怎么计算？ | LLM学习记录</title>
    <meta name="generator" content="VuePress 1.9.10">
    
    <meta name="description" content="记录大模型学习笔记～">
    
    <link rel="preload" href="./assets/css/0.styles.9d97ee6f.css" as="style"><link rel="preload" href="./assets/js/app.427a5c72.js" as="script"><link rel="preload" href="./assets/js/7.7c04b33d.js" as="script"><link rel="preload" href="./assets/js/2.fc98b07a.js" as="script"><link rel="preload" href="./assets/js/1.227b7bc4.js" as="script"><link rel="preload" href="./assets/js/39.01cf69c5.js" as="script"><link rel="prefetch" href="./assets/js/10.dfb0299a.js"><link rel="prefetch" href="./assets/js/11.479e3941.js"><link rel="prefetch" href="./assets/js/14.c5aa2906.js"><link rel="prefetch" href="./assets/js/15.4691ff28.js"><link rel="prefetch" href="./assets/js/16.520dd9c6.js"><link rel="prefetch" href="./assets/js/17.85dc844f.js"><link rel="prefetch" href="./assets/js/18.060d220d.js"><link rel="prefetch" href="./assets/js/19.d0231ffe.js"><link rel="prefetch" href="./assets/js/20.02ef4689.js"><link rel="prefetch" href="./assets/js/21.4913a81b.js"><link rel="prefetch" href="./assets/js/22.ec2bffa8.js"><link rel="prefetch" href="./assets/js/23.360d4cb3.js"><link rel="prefetch" href="./assets/js/24.8df16635.js"><link rel="prefetch" href="./assets/js/25.bda1b3e1.js"><link rel="prefetch" href="./assets/js/26.06377f0d.js"><link rel="prefetch" href="./assets/js/27.86112ea9.js"><link rel="prefetch" href="./assets/js/28.be8ca15b.js"><link rel="prefetch" href="./assets/js/29.98f0eb72.js"><link rel="prefetch" href="./assets/js/3.be741a74.js"><link rel="prefetch" href="./assets/js/30.4c7947d3.js"><link rel="prefetch" href="./assets/js/31.8e410bcb.js"><link rel="prefetch" href="./assets/js/32.bac8f00d.js"><link rel="prefetch" href="./assets/js/33.cb9d2429.js"><link rel="prefetch" href="./assets/js/34.380ce34c.js"><link rel="prefetch" href="./assets/js/35.b5fcd8bd.js"><link rel="prefetch" href="./assets/js/36.34421d06.js"><link rel="prefetch" href="./assets/js/37.d64318d3.js"><link rel="prefetch" href="./assets/js/38.54e3932b.js"><link rel="prefetch" href="./assets/js/4.38e5a85b.js"><link rel="prefetch" href="./assets/js/40.35b9d7c3.js"><link rel="prefetch" href="./assets/js/41.92a67a72.js"><link rel="prefetch" href="./assets/js/42.3b5222cb.js"><link rel="prefetch" href="./assets/js/43.7bd6dc89.js"><link rel="prefetch" href="./assets/js/5.1a3770ee.js"><link rel="prefetch" href="./assets/js/6.3951b5be.js"><link rel="prefetch" href="./assets/js/8.617b6635.js"><link rel="prefetch" href="./assets/js/9.def61537.js"><link rel="prefetch" href="./assets/js/vendors~docsearch.86fb114e.js">
    <link rel="stylesheet" href="./assets/css/0.styles.9d97ee6f.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-7dd95ae2><div data-v-7dd95ae2><div class="password-shadow password-wrapper-out" style="display:none;" data-v-59e6cb88 data-v-7dd95ae2 data-v-7dd95ae2><h3 class="title" data-v-59e6cb88>LLM学习记录</h3> <p class="description" data-v-59e6cb88>记录大模型学习笔记～</p> <label id="box" class="inputBox" data-v-59e6cb88><input type="password" value="" data-v-59e6cb88> <span data-v-59e6cb88>Konck! Knock!</span> <button data-v-59e6cb88>OK</button></label> <div class="footer" data-v-59e6cb88><span data-v-59e6cb88><i class="iconfont reco-theme" data-v-59e6cb88></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-59e6cb88>vuePress-theme-reco</a></span> <span data-v-59e6cb88><i class="iconfont reco-copyright" data-v-59e6cb88></i> <a data-v-59e6cb88><!---->
          
        <!---->
        2024
      </a></span></div></div> <div class="hide" data-v-7dd95ae2><header class="navbar" data-v-7dd95ae2><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/./" class="home-link router-link-active"><!----> <span class="site-name">LLM学习记录</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/./" class="nav-link"><i class="undefined"></i>
  首页
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      杨锁择的 LLM 博客
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/YangSuoze" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-7dd95ae2></div> <aside class="sidebar" data-v-7dd95ae2><div class="personal-info-wrapper" data-v-1fad0c41 data-v-7dd95ae2><!----> <!----> <div class="num" data-v-1fad0c41><div data-v-1fad0c41><h3 data-v-1fad0c41>6</h3> <h6 data-v-1fad0c41>Articles</h6></div> <div data-v-1fad0c41><h3 data-v-1fad0c41>0</h3> <h6 data-v-1fad0c41>Tags</h6></div></div> <ul class="social-links" data-v-1fad0c41></ul> <hr data-v-1fad0c41></div> <nav class="nav-links"><div class="nav-item"><a href="/./" class="nav-link"><i class="undefined"></i>
  首页
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      杨锁择的 LLM 博客
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/YangSuoze" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><a href="/./" class="sidebar-heading clickable router-link-active"><span>欢迎学习</span> <!----></a> <ul class="sidebar-links sidebar-group-items"><li><a href="/./" aria-current="page" class="sidebar-link">Hello</a></li></ul></section></li><li><section class="sidebar-group depth-0"><a href="/./LLM基础/transformer" class="sidebar-heading clickable open"><span>基础学习</span> <!----></a> <ul class="sidebar-links sidebar-group-items"><li><a href="/./LLM基础/transformer.html" class="sidebar-link">transformer</a></li><li><a href="/./LLM基础/deepspeed.html" class="sidebar-link">deepspeed</a></li><li><a href="/./LLM基础/pretrained1.html" class="active sidebar-link">大模型的loss怎么计算？</a></li><li><a href="/./LLM基础/使用ollama快速部署LLM.html" class="sidebar-link">使用ollama快速部署LLM</a></li><li><a href="/./LLM基础/多智能体编排.html" class="sidebar-link">多智能体编排</a></li></ul></section></li></ul> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-59e6cb88 data-v-7dd95ae2><h3 class="title" data-v-59e6cb88>大模型的loss怎么计算？</h3> <!----> <label id="box" class="inputBox" data-v-59e6cb88><input type="password" value="" data-v-59e6cb88> <span data-v-59e6cb88>Konck! Knock!</span> <button data-v-59e6cb88>OK</button></label> <div class="footer" data-v-59e6cb88><span data-v-59e6cb88><i class="iconfont reco-theme" data-v-59e6cb88></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-59e6cb88>vuePress-theme-reco</a></span> <span data-v-59e6cb88><i class="iconfont reco-copyright" data-v-59e6cb88></i> <a data-v-59e6cb88><!---->
          
        <!---->
        2024
      </a></span></div></div> <div data-v-7dd95ae2><div data-v-7dd95ae2><main class="page" style="padding-right:0;"><section style="display:;"><div class="page-title"><h1 class="title">大模型的loss怎么计算？</h1> <div data-v-8a445198><i class="iconfont reco-account" data-v-8a445198><span data-v-8a445198>Jessica</span></i> <i class="iconfont reco-date" data-v-8a445198><span data-v-8a445198>4/22/2024</span></i> <!----> <!----></div></div> <div class="theme-reco-content content__default"><h2 id="大模型预训练的几种方式"><a href="#大模型预训练的几种方式" class="header-anchor">#</a> 大模型预训练的几种方式</h2> <p>常用 的预训练任务主要分为三类，包括<strong>语言建模(Language Modeling, LM)、去噪自编 码(Denoising Autoencoding, DAE)以及混合去噪器(Mixture-of-Denoisers, MoD)</strong>。</p> <h2 id="语言建模"><a href="#语言建模" class="header-anchor">#</a> 语言建模</h2> <ul><li>在前缀解码器中，仅后缀中的词元损失会被计入总损失。</li> <li>语言建模的另一个重要变种是中间填充任务：一个输入 序列 𝒖 被划分为三个部分:前缀 𝒖prefix、中间部分 𝒖middle 和后缀 𝒖suffix。随后，中 间部分被移至序列末尾。因此，模型需要自回归地对新序列 𝒖prefix ⊕ 𝒖suffix ⊕ 𝒖middle 进行预测。——这种方法使得模型具备对于文本中间部分内容的恢复能力。这 种预训练任务经常被用于训练代码预训练模型，从而提升模型在代码补全等实际 应用场景中的表现。</li> <li>去噪自编码：加入mask噪声的模型，如bert族，目标是预测mask对应的真实标签</li></ul> <h2 id="how-do-llms-generate-text"><a href="#how-do-llms-generate-text" class="header-anchor">#</a> How do LLMs generate text?</h2> <p><img src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*tx6Dl7z7Cv0vQnvG7KVVIA.png" alt="alt text"></p> <p><code>In general terms, shown in Figure 1, LLMs take in a sequence of tokens as input, pass them through its model layers, and output a sequence of logits. Logits would have the size of its vocabulary. Using a Softmax function, we can convert the logits into multiclass probability. “Class” in this case would be tokens or “words” in the model’s vocabulary.</code></p> <p><img src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*I4oDEe_29uRnwnha_MQDrw.png" alt="alt text"></p> <p><code>This word probability indicates how likely the model “thinks” the word should be generated at that position. A trivial way of generation would be “greedy decoding” where the token with the highest probability is generated.</code></p> <h2 id="how-are-llms-pre-trained-what-is-causal-language-modeling"><a href="#how-are-llms-pre-trained-what-is-causal-language-modeling" class="header-anchor">#</a> How are LLMs pre-trained? What is Causal Language Modeling?</h2> <p>In CLM, the model will be trained to predict the next token or word in a sequence based on preceding tokens. Thus, during CLM, True labels are taken as the input tokens shifted to the left by 1 position.</p> <p><img src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*zyhNiKkp0I4cdSeu-IX95A.png" alt=""></p> <p>For example, if our input text is “&lt;s&gt; I love dogs not cats”, our true labels will be “I love dogs not cats” which are shifted from the input.</p> <p>Loss is calculated from this comparison of output and true labels. The model will then learn from this loss. Essentially, we are measuring the model’s ability to predict the next word.</p> <h2 id="loss-visualisation"><a href="#loss-visualisation" class="header-anchor">#</a> Loss Visualisation!</h2> <p>There are many loss functions available but we will discuss the Cross Entropy Loss in this article. As seen in Figure 1, the softmax function will output a multiclass probability vector for each position of the sequence. This probability vector will be used in the calculation of the loss in the following way.</p> <p><img src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*YWTag9DZv1KERG3bZqA-sA.png" alt="Definition of mean aggregated Cross Entropy"></p> <h3 id="step1"><a href="#step1" class="header-anchor">#</a> Step1</h3> <p><img src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*acIWUIqD0_SXGyZ1weeezg.png" alt="">
The vocab token index of true labels will have to be extracted. As shown in Figure, referencing the Vocab table, the true label at position 0 “I” would have an index of 1. True label at position 1 “love” would have an index of 4.</p> <h3 id="step2"><a href="#step2" class="header-anchor">#</a> Step2</h3> <p><img src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*5BSM35GrOwuYcHitD1NY4g.png" alt=""></p> <h3 id="step3"><a href="#step3" class="header-anchor">#</a> Step3</h3> <p>Once we have the softmax probabilities, we will apply a negative log and aggregate them by taking the mean. From the example in Figure, our Cross Entropy loss will be mean([-log(0.5), -log(0.3),-log(0.33),-log(0.45),-log(0.6)]) = 0.86302.</p> <p>CE loss is measuring if the model gives the true “next word” a high probability. The lower the loss, the higher the model probability of the true “next word”.</p> <h2 id="how-it-looks-like-in-code"><a href="#how-it-looks-like-in-code" class="header-anchor">#</a> How it looks like in code!</h2> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> transformers
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> CrossEntropyLoss

<span class="token comment"># personal huggingface access token will be needed to use Llama2 models</span>
ACCESS_TOKEN_WRITE <span class="token operator">=</span> <span class="token comment"># hugging face access token</span>
cache_dir_local <span class="token operator">=</span> <span class="token comment"># dir for cache</span>

BASE_MODEL <span class="token operator">=</span> <span class="token string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>
<span class="token comment"># initialise tokenizer</span>
llama2_tokenizer <span class="token operator">=</span> \
AutoTokenizer<span class="token punctuation">.</span>\
from_pretrained<span class="token punctuation">(</span>BASE_MODEL<span class="token punctuation">,</span> 
                padding_side<span class="token operator">=</span><span class="token string">&quot;left&quot;</span><span class="token punctuation">,</span> 
                use_auth_token<span class="token operator">=</span>ACCESS_TOKEN_WRITE<span class="token punctuation">,</span>
               cache_dir<span class="token operator">=</span>cache_dir_local<span class="token punctuation">)</span>

<span class="token comment"># initialise model</span>
llama2_model <span class="token operator">=</span> \
AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    BASE_MODEL<span class="token punctuation">,</span> 
    use_cache<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">&quot;auto&quot;</span><span class="token punctuation">,</span>
    cache_dir<span class="token operator">=</span>cache_dir_local<span class="token punctuation">,</span>
    use_auth_token<span class="token operator">=</span>ACCESS_TOKEN_WRITE<span class="token punctuation">)</span>

<span class="token comment"># example input text</span>
text_input <span class="token operator">=</span> <span class="token string">'I love dogs not cats'</span>
toks <span class="token operator">=</span> llama2_tokenizer<span class="token punctuation">(</span>text_input<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">)</span>
<span class="token comment"># labels of CLM can be taken as inputs, huggingface will handle the shifting of the labels as mentioned above.</span>
toks<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span> <span class="token operator">=</span> toks<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span>

out <span class="token operator">=</span> llama2_model<span class="token punctuation">(</span><span class="token operator">**</span>toks<span class="token punctuation">,</span> output_hidden_states<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># tensor(4.6854, grad_fn=&lt;NllLossBackward0&gt;)</span>
</code></pre></div><p>Now let’s try to calculate the loss manually.</p> <div class="language-python extra-class"><pre class="language-python"><code>logits <span class="token operator">=</span> out<span class="token punctuation">[</span><span class="token string">'logits'</span><span class="token punctuation">]</span>
labels <span class="token operator">=</span> toks<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span>
<span class="token comment"># following model wrapper in huggingface</span>
shift_logits <span class="token operator">=</span> logits<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
shift_labels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># flatten batch</span>
shift_logits <span class="token operator">=</span> shift_logits<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32000</span><span class="token punctuation">)</span>
shift_labels <span class="token operator">=</span> shift_labels<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>shift_labels<span class="token punctuation">)</span>
<span class="token comment"># tensor([  306,  5360, 26361,   451,   274,  1446])</span>
</code></pre></div><p>First, we extract the token indexes of the true labels [ 306, 5360, 26361, 451, 274, 1446].</p> <div class="language-python extra-class"><pre class="language-python"><code>softmax_prob_at_label_tokidx <span class="token operator">=</span> \
torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>shift_logits<span class="token punctuation">)</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>shift_labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> shift_labels<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>softmax_prob_at_label_tokidx<span class="token punctuation">)</span>
<span class="token comment"># tensor([1.0475e-02, 1.5564e-02, 8.7919e-04, 1.5152e-04, 2.8680e-02, 9.9225e-01],</span>
<span class="token comment"># grad_fn=&lt;IndexBackward0&gt;)</span>
</code></pre></div><p>Next, we apply the softmax function to the output logits and take the probability at the true label token index as shown in the comments.</p> <div class="language-python extra-class"><pre class="language-python"><code>loss <span class="token operator">=</span> <span class="token punctuation">(</span>softmax_prob_at_label_tokidx<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
<span class="token comment"># tensor(4.6854, grad_fn=&lt;MeanBackward0&gt;)</span>

<span class="token comment"># double checking with cross entropy function</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>shift_logits<span class="token punctuation">,</span> shift_labels<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># tensor(4.6854, grad_fn=&lt;NllLossBackward0&gt;)</span>
</code></pre></div><p>Finally, we apply the negative log and mean aggregation and see that the loss calculated manually is also 4.6854.</p></div></section> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev"><a href="/./LLM基础/deepspeed.html" class="prev">
          deepspeed
        </a></span> <span class="next"><a href="/./LLM基础/使用ollama快速部署LLM.html">
          使用ollama快速部署LLM
        </a></span></p></div> <div class="comments-wrapper"><!----></div></main></div> <!----></div> <ul class="sub-sidebar sub-sidebar-wrapper" style="width:0;" data-v-b57cc07c data-v-7dd95ae2></ul></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="./assets/js/app.427a5c72.js" defer></script><script src="./assets/js/7.7c04b33d.js" defer></script><script src="./assets/js/2.fc98b07a.js" defer></script><script src="./assets/js/1.227b7bc4.js" defer></script><script src="./assets/js/39.01cf69c5.js" defer></script>
  </body>
</html>
