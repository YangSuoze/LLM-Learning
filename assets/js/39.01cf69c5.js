(window.webpackJsonp=window.webpackJsonp||[]).push([[39],{448:function(t,s,a){"use strict";a.r(s);var e=a(2),n=Object(e.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"大模型预训练的几种方式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#大模型预训练的几种方式"}},[t._v("#")]),t._v(" 大模型预训练的几种方式")]),t._v(" "),s("p",[t._v("常用 的预训练任务主要分为三类，包括"),s("strong",[t._v("语言建模(Language Modeling, LM)、去噪自编 码(Denoising Autoencoding, DAE)以及混合去噪器(Mixture-of-Denoisers, MoD)")]),t._v("。")]),t._v(" "),s("h2",{attrs:{id:"语言建模"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#语言建模"}},[t._v("#")]),t._v(" 语言建模")]),t._v(" "),s("ul",[s("li",[t._v("在前缀解码器中，仅后缀中的词元损失会被计入总损失。")]),t._v(" "),s("li",[t._v("语言建模的另一个重要变种是中间填充任务：一个输入 序列 𝒖 被划分为三个部分:前缀 𝒖prefix、中间部分 𝒖middle 和后缀 𝒖suffix。随后，中 间部分被移至序列末尾。因此，模型需要自回归地对新序列 𝒖prefix ⊕ 𝒖suffix ⊕ 𝒖middle 进行预测。——这种方法使得模型具备对于文本中间部分内容的恢复能力。这 种预训练任务经常被用于训练代码预训练模型，从而提升模型在代码补全等实际 应用场景中的表现。")]),t._v(" "),s("li",[t._v("去噪自编码：加入mask噪声的模型，如bert族，目标是预测mask对应的真实标签")])]),t._v(" "),s("h2",{attrs:{id:"how-do-llms-generate-text"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#how-do-llms-generate-text"}},[t._v("#")]),t._v(" How do LLMs generate text?")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*tx6Dl7z7Cv0vQnvG7KVVIA.png",alt:"alt text"}})]),t._v(" "),s("p",[s("code",[t._v("In general terms, shown in Figure 1, LLMs take in a sequence of tokens as input, pass them through its model layers, and output a sequence of logits. Logits would have the size of its vocabulary. Using a Softmax function, we can convert the logits into multiclass probability. “Class” in this case would be tokens or “words” in the model’s vocabulary.")])]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*I4oDEe_29uRnwnha_MQDrw.png",alt:"alt text"}})]),t._v(" "),s("p",[s("code",[t._v("This word probability indicates how likely the model “thinks” the word should be generated at that position. A trivial way of generation would be “greedy decoding” where the token with the highest probability is generated.")])]),t._v(" "),s("h2",{attrs:{id:"how-are-llms-pre-trained-what-is-causal-language-modeling"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#how-are-llms-pre-trained-what-is-causal-language-modeling"}},[t._v("#")]),t._v(" How are LLMs pre-trained? What is Causal Language Modeling?")]),t._v(" "),s("p",[t._v("In CLM, the model will be trained to predict the next token or word in a sequence based on preceding tokens. Thus, during CLM, True labels are taken as the input tokens shifted to the left by 1 position.")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*zyhNiKkp0I4cdSeu-IX95A.png",alt:""}})]),t._v(" "),s("p",[t._v("For example, if our input text is “<s> I love dogs not cats”, our true labels will be “I love dogs not cats” which are shifted from the input.")]),t._v(" "),s("p",[t._v("Loss is calculated from this comparison of output and true labels. The model will then learn from this loss. Essentially, we are measuring the model’s ability to predict the next word.")]),t._v(" "),s("h2",{attrs:{id:"loss-visualisation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#loss-visualisation"}},[t._v("#")]),t._v(" Loss Visualisation!")]),t._v(" "),s("p",[t._v("There are many loss functions available but we will discuss the Cross Entropy Loss in this article. As seen in Figure 1, the softmax function will output a multiclass probability vector for each position of the sequence. This probability vector will be used in the calculation of the loss in the following way.")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:1352/format:webp/1*YWTag9DZv1KERG3bZqA-sA.png",alt:"Definition of mean aggregated Cross Entropy"}})]),t._v(" "),s("h3",{attrs:{id:"step1"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step1"}},[t._v("#")]),t._v(" Step1")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*acIWUIqD0_SXGyZ1weeezg.png",alt:""}}),t._v("\nThe vocab token index of true labels will have to be extracted. As shown in Figure, referencing the Vocab table, the true label at position 0 “I” would have an index of 1. True label at position 1 “love” would have an index of 4.")]),t._v(" "),s("h3",{attrs:{id:"step2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step2"}},[t._v("#")]),t._v(" Step2")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*5BSM35GrOwuYcHitD1NY4g.png",alt:""}})]),t._v(" "),s("h3",{attrs:{id:"step3"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step3"}},[t._v("#")]),t._v(" Step3")]),t._v(" "),s("p",[t._v("Once we have the softmax probabilities, we will apply a negative log and aggregate them by taking the mean. From the example in Figure, our Cross Entropy loss will be mean([-log(0.5), -log(0.3),-log(0.33),-log(0.45),-log(0.6)]) = 0.86302.")]),t._v(" "),s("p",[t._v("CE loss is measuring if the model gives the true “next word” a high probability. The lower the loss, the higher the model probability of the true “next word”.")]),t._v(" "),s("h2",{attrs:{id:"how-it-looks-like-in-code"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#how-it-looks-like-in-code"}},[t._v("#")]),t._v(" How it looks like in code!")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" transformers\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoModelForCausalLM"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AutoTokenizer\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" CrossEntropyLoss\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# personal huggingface access token will be needed to use Llama2 models")]),t._v("\nACCESS_TOKEN_WRITE "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# hugging face access token")]),t._v("\ncache_dir_local "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# dir for cache")]),t._v("\n\nBASE_MODEL "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"meta-llama/Llama-2-7b-hf"')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# initialise tokenizer")]),t._v("\nllama2_tokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" \\\nAutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\nfrom_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("BASE_MODEL"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                padding_side"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"left"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                use_auth_token"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ACCESS_TOKEN_WRITE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               cache_dir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cache_dir_local"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# initialise model")]),t._v("\nllama2_model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" \\\nAutoModelForCausalLM"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    BASE_MODEL"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    use_cache"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    device_map"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"auto"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    cache_dir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cache_dir_local"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    use_auth_token"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ACCESS_TOKEN_WRITE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# example input text")]),t._v("\ntext_input "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'I love dogs not cats'")]),t._v("\ntoks "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" llama2_tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text_input"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" return_tensors"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# labels of CLM can be taken as inputs, huggingface will handle the shifting of the labels as mentioned above.")]),t._v("\ntoks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" toks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'input_ids'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nout "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" llama2_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("toks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" output_hidden_states"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" return_dict"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loss'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensor(4.6854, grad_fn=<NllLossBackward0>)")]),t._v("\n")])])]),s("p",[t._v("Now let’s try to calculate the loss manually.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("logits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'logits'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nlabels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" toks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# following model wrapper in huggingface")]),t._v("\nshift_logits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" logits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("contiguous"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nshift_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("contiguous"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# flatten batch")]),t._v("\nshift_logits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" shift_logits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nshift_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" shift_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shift_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensor([  306,  5360, 26361,   451,   274,  1446])")]),t._v("\n")])])]),s("p",[t._v("First, we extract the token indexes of the true labels [ 306, 5360, 26361, 451, 274, 1446].")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("softmax_prob_at_label_tokidx "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" \\\ntorch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Softmax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shift_logits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arange"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shift_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shift_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("softmax_prob_at_label_tokidx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensor([1.0475e-02, 1.5564e-02, 8.7919e-04, 1.5152e-04, 2.8680e-02, 9.9225e-01],")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# grad_fn=<IndexBackward0>)")]),t._v("\n")])])]),s("p",[t._v("Next, we apply the softmax function to the output logits and take the probability at the true label token index as shown in the comments.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("softmax_prob_at_label_tokidx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensor(4.6854, grad_fn=<MeanBackward0>)")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# double checking with cross entropy function")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("CrossEntropyLoss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shift_logits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shift_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensor(4.6854, grad_fn=<NllLossBackward0>)")]),t._v("\n")])])]),s("p",[t._v("Finally, we apply the negative log and mean aggregation and see that the loss calculated manually is also 4.6854.")])])}),[],!1,null,null,null);s.default=n.exports}}]);