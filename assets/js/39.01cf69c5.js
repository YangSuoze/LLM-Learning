(window.webpackJsonp=window.webpackJsonp||[]).push([[39],{448:function(t,s,a){"use strict";a.r(s);var e=a(2),n=Object(e.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"å¤§æ¨¡å‹é¢„è®­ç»ƒçš„å‡ ç§æ–¹å¼"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#å¤§æ¨¡å‹é¢„è®­ç»ƒçš„å‡ ç§æ–¹å¼"}},[t._v("#")]),t._v(" å¤§æ¨¡å‹é¢„è®­ç»ƒçš„å‡ ç§æ–¹å¼")]),t._v(" "),s("p",[t._v("å¸¸ç”¨ çš„é¢„è®­ç»ƒä»»åŠ¡ä¸»è¦åˆ†ä¸ºä¸‰ç±»ï¼ŒåŒ…æ‹¬"),s("strong",[t._v("è¯­è¨€å»ºæ¨¡(Language Modeling, LM)ã€å»å™ªè‡ªç¼– ç (Denoising Autoencoding, DAE)ä»¥åŠæ··åˆå»å™ªå™¨(Mixture-of-Denoisers, MoD)")]),t._v("ã€‚")]),t._v(" "),s("h2",{attrs:{id:"è¯­è¨€å»ºæ¨¡"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#è¯­è¨€å»ºæ¨¡"}},[t._v("#")]),t._v(" è¯­è¨€å»ºæ¨¡")]),t._v(" "),s("ul",[s("li",[t._v("åœ¨å‰ç¼€è§£ç å™¨ä¸­ï¼Œä»…åç¼€ä¸­çš„è¯å…ƒæŸå¤±ä¼šè¢«è®¡å…¥æ€»æŸå¤±ã€‚")]),t._v(" "),s("li",[t._v("è¯­è¨€å»ºæ¨¡çš„å¦ä¸€ä¸ªé‡è¦å˜ç§æ˜¯ä¸­é—´å¡«å……ä»»åŠ¡ï¼šä¸€ä¸ªè¾“å…¥ åºåˆ— ğ’– è¢«åˆ’åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†:å‰ç¼€ ğ’–prefixã€ä¸­é—´éƒ¨åˆ† ğ’–middle å’Œåç¼€ ğ’–suffixã€‚éšåï¼Œä¸­ é—´éƒ¨åˆ†è¢«ç§»è‡³åºåˆ—æœ«å°¾ã€‚å› æ­¤ï¼Œæ¨¡å‹éœ€è¦è‡ªå›å½’åœ°å¯¹æ–°åºåˆ— ğ’–prefix âŠ• ğ’–suffix âŠ• ğ’–middle è¿›è¡Œé¢„æµ‹ã€‚â€”â€”è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹å…·å¤‡å¯¹äºæ–‡æœ¬ä¸­é—´éƒ¨åˆ†å†…å®¹çš„æ¢å¤èƒ½åŠ›ã€‚è¿™ ç§é¢„è®­ç»ƒä»»åŠ¡ç»å¸¸è¢«ç”¨äºè®­ç»ƒä»£ç é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨ä»£ç è¡¥å…¨ç­‰å®é™… åº”ç”¨åœºæ™¯ä¸­çš„è¡¨ç°ã€‚")]),t._v(" "),s("li",[t._v("å»å™ªè‡ªç¼–ç ï¼šåŠ å…¥maskå™ªå£°çš„æ¨¡å‹ï¼Œå¦‚bertæ—ï¼Œç›®æ ‡æ˜¯é¢„æµ‹maskå¯¹åº”çš„çœŸå®æ ‡ç­¾")])]),t._v(" "),s("h2",{attrs:{id:"how-do-llms-generate-text"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#how-do-llms-generate-text"}},[t._v("#")]),t._v(" How do LLMs generate text?")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*tx6Dl7z7Cv0vQnvG7KVVIA.png",alt:"alt text"}})]),t._v(" "),s("p",[s("code",[t._v("In general terms, shown in Figure 1, LLMs take in a sequence of tokens as input, pass them through its model layers, and output a sequence of logits. Logits would have the size of its vocabulary. Using a Softmax function, we can convert the logits into multiclass probability. â€œClassâ€ in this case would be tokens or â€œwordsâ€ in the modelâ€™s vocabulary.")])]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*I4oDEe_29uRnwnha_MQDrw.png",alt:"alt text"}})]),t._v(" "),s("p",[s("code",[t._v("This word probability indicates how likely the model â€œthinksâ€ the word should be generated at that position. A trivial way of generation would be â€œgreedy decodingâ€ where the token with the highest probability is generated.")])]),t._v(" "),s("h2",{attrs:{id:"how-are-llms-pre-trained-what-is-causal-language-modeling"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#how-are-llms-pre-trained-what-is-causal-language-modeling"}},[t._v("#")]),t._v(" How are LLMs pre-trained? What is Causal Language Modeling?")]),t._v(" "),s("p",[t._v("In CLM, the model will be trained to predict the next token or word in a sequence based on preceding tokens. Thus, during CLM, True labels are taken as the input tokens shifted to the left by 1 position.")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*zyhNiKkp0I4cdSeu-IX95A.png",alt:""}})]),t._v(" "),s("p",[t._v("For example, if our input text is â€œ<s> I love dogs not catsâ€, our true labels will be â€œI love dogs not catsâ€ which are shifted from the input.")]),t._v(" "),s("p",[t._v("Loss is calculated from this comparison of output and true labels. The model will then learn from this loss. Essentially, we are measuring the modelâ€™s ability to predict the next word.")]),t._v(" "),s("h2",{attrs:{id:"loss-visualisation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#loss-visualisation"}},[t._v("#")]),t._v(" Loss Visualisation!")]),t._v(" "),s("p",[t._v("There are many loss functions available but we will discuss the Cross Entropy Loss in this article. As seen in Figure 1, the softmax function will output a multiclass probability vector for each position of the sequence. This probability vector will be used in the calculation of the loss in the following way.")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:1352/format:webp/1*YWTag9DZv1KERG3bZqA-sA.png",alt:"Definition of mean aggregated Cross Entropy"}})]),t._v(" "),s("h3",{attrs:{id:"step1"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step1"}},[t._v("#")]),t._v(" Step1")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*acIWUIqD0_SXGyZ1weeezg.png",alt:""}}),t._v("\nThe vocab token index of true labels will have to be extracted. As shown in Figure, referencing the Vocab table, the true label at position 0 â€œIâ€ would have an index of 1. True label at position 1 â€œloveâ€ would have an index of 4.")]),t._v(" "),s("h3",{attrs:{id:"step2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step2"}},[t._v("#")]),t._v(" Step2")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*5BSM35GrOwuYcHitD1NY4g.png",alt:""}})]),t._v(" "),s("h3",{attrs:{id:"step3"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step3"}},[t._v("#")]),t._v(" Step3")]),t._v(" "),s("p",[t._v("Once we have the softmax probabilities, we will apply a negative log and aggregate them by taking the mean. From the example in Figure, our Cross Entropy loss will be mean([-log(0.5), -log(0.3),-log(0.33),-log(0.45),-log(0.6)]) = 0.86302.")]),t._v(" "),s("p",[t._v("CE loss is measuring if the model gives the true â€œnext wordâ€ a high probability. The lower the loss, the higher the model probability of the true â€œnext wordâ€.")]),t._v(" "),s("h2",{attrs:{id:"how-it-looks-like-in-code"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#how-it-looks-like-in-code"}},[t._v("#")]),t._v(" How it looks like in code!")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" transformers\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoModelForCausalLM"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AutoTokenizer\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" CrossEntropyLoss\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# personal huggingface access token will be needed to use Llama2 models")]),t._v("\nACCESS_TOKEN_WRITE "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# hugging face access token")]),t._v("\ncache_dir_local "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# dir for cache")]),t._v("\n\nBASE_MODEL "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"meta-llama/Llama-2-7b-hf"')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# initialise tokenizer")]),t._v("\nllama2_tokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" \\\nAutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\nfrom_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("BASE_MODEL"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                padding_side"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"left"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                use_auth_token"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ACCESS_TOKEN_WRITE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               cache_dir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cache_dir_local"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# initialise model")]),t._v("\nllama2_model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" \\\nAutoModelForCausalLM"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    BASE_MODEL"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    use_cache"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    device_map"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"auto"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    cache_dir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cache_dir_local"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    use_auth_token"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ACCESS_TOKEN_WRITE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# example input text")]),t._v("\ntext_input "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'I love dogs not cats'")]),t._v("\ntoks "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" llama2_tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text_input"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" return_tensors"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# labels of CLM can be taken as inputs, huggingface will handle the shifting of the labels as mentioned above.")]),t._v("\ntoks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" toks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'input_ids'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nout "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" llama2_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("toks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" output_hidden_states"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" return_dict"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'loss'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensor(4.6854, grad_fn=<NllLossBackward0>)")]),t._v("\n")])])]),s("p",[t._v("Now letâ€™s try to calculate the loss manually.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("logits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'logits'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nlabels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" toks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# following model wrapper in huggingface")]),t._v("\nshift_logits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" logits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("contiguous"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nshift_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("contiguous"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# flatten batch")]),t._v("\nshift_logits "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" shift_logits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nshift_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" shift_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shift_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensor([  306,  5360, 26361,   451,   274,  1446])")]),t._v("\n")])])]),s("p",[t._v("First, we extract the token indexes of the true labels [ 306, 5360, 26361, 451, 274, 1446].")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("softmax_prob_at_label_tokidx "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" \\\ntorch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Softmax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shift_logits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arange"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shift_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shift_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("softmax_prob_at_label_tokidx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensor([1.0475e-02, 1.5564e-02, 8.7919e-04, 1.5152e-04, 2.8680e-02, 9.9225e-01],")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# grad_fn=<IndexBackward0>)")]),t._v("\n")])])]),s("p",[t._v("Next, we apply the softmax function to the output logits and take the probability at the true label token index as shown in the comments.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("softmax_prob_at_label_tokidx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensor(4.6854, grad_fn=<MeanBackward0>)")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# double checking with cross entropy function")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("CrossEntropyLoss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shift_logits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shift_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensor(4.6854, grad_fn=<NllLossBackward0>)")]),t._v("\n")])])]),s("p",[t._v("Finally, we apply the negative log and mean aggregation and see that the loss calculated manually is also 4.6854.")])])}),[],!1,null,null,null);s.default=n.exports}}]);